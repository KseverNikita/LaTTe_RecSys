{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVoK_7xHdaa9",
    "outputId": "5068028f-6526-41db-ab84-9989d1f39bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polara\n",
      "  Cloning https://github.com/evfro/polara.git (to revision develop) to /tmp/pip-install-vgxz7xdd/polara_16e3846ffa8a4d8e9f0a0d792f501cf5\n",
      "  Running command git clone -q https://github.com/evfro/polara.git /tmp/pip-install-vgxz7xdd/polara_16e3846ffa8a4d8e9f0a0d792f501cf5\n",
      "  Running command git checkout -b develop --track origin/develop\n",
      "  Switched to a new branch 'develop'\n",
      "  Branch 'develop' set up to track remote branch 'develop' from 'origin'.\n",
      "  Resolved https://github.com/evfro/polara.git to commit 4de4ca7d6f901e32f1e045f190bcb09587162397\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --no-cache-dir --upgrade git+https://github.com/evfro/polara.git@develop#egg=polara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:11:20.346846Z",
     "start_time": "2022-03-24T12:11:18.564058Z"
    },
    "id": "o_HJgdQFdSaf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import polara\n",
    "from polara import get_movielens_data\n",
    "from polara.preprocessing.dataframes import leave_one_out, reindex\n",
    "\n",
    "from dataprep import transform_indices\n",
    "from evaluation import topn_recommendations, model_evaluate, downvote_seen_items\n",
    "\n",
    "from polara.lib.tensor import hooi\n",
    "from polara.lib.sparse import tensor_outer_at\n",
    "from polara.evaluation.pipelines import random_grid\n",
    "\n",
    "from sa_hooi import sa_hooi, form_attention_matrix, get_scaling_weights, generate_position_projector\n",
    "\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "from scipy.sparse.linalg import norm, svds\n",
    "from scipy.linalg import solve_triangular, sqrtm\n",
    "\n",
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movielens 10m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NbP1tmBdSas"
   },
   "source": [
    "# Data preproccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_preproccessing():\n",
    "    data = get_movielens_data(\"ml-10m.zip\", include_time=True)\n",
    "    test_timepoint = data['timestamp'].quantile(\n",
    "    q=0.8, interpolation='nearest'\n",
    "    )\n",
    "    \n",
    "    if (data[\"rating\"].nunique() > 5):\n",
    "        data[\"rating\"] = (data[\"rating\"] * 2).astype(int)\n",
    "\n",
    "    test_data_ = data.query('timestamp >= @test_timepoint')\n",
    "    train_data_ = data.query(\n",
    "    'userid not in @test_data_.userid.unique() and timestamp < @test_timepoint'\n",
    "    )\n",
    "    \n",
    "    training, data_index = transform_indices(train_data_.copy(), 'userid', 'movieid')\n",
    "    test_data = reindex(test_data_, data_index['items'])\n",
    "\n",
    "    testset_, holdout_ = leave_one_out(\n",
    "    test_data, target='timestamp', sample_top=True, random_state=0\n",
    "    )\n",
    "    testset_valid_, holdout_valid_ = leave_one_out(\n",
    "        testset_, target='timestamp', sample_top=True, random_state=0\n",
    "    )\n",
    "\n",
    "    test_users_val = np.intersect1d(testset_valid_.userid.unique(), holdout_valid_.userid.unique())\n",
    "    testset_valid = testset_valid_.query('userid in @test_users_val').sort_values('userid')\n",
    "    holdout_valid = holdout_valid_.query('userid in @test_users_val').sort_values('userid')\n",
    "\n",
    "    test_users = np.intersect1d(testset_.userid.unique(), holdout_.userid.unique())\n",
    "    testset = testset_.query('userid in @test_users').sort_values('userid')\n",
    "    holdout = holdout_.query('userid in @test_users').sort_values('userid')\n",
    "\n",
    "\n",
    "    assert holdout_valid.set_index('userid')['timestamp'].ge(\n",
    "        testset_valid\n",
    "        .groupby('userid')\n",
    "        ['timestamp'].max()\n",
    "    ).all()\n",
    "\n",
    "    data_description = dict(\n",
    "        users = data_index['users'].name,\n",
    "        items = data_index['items'].name,\n",
    "        feedback = 'rating',\n",
    "        n_users = len(data_index['users']),\n",
    "        n_items = len(data_index['items']),\n",
    "        n_ratings = training['rating'].nunique(),\n",
    "        min_rating = training['rating'].min(),\n",
    "        test_users = holdout_valid[data_index['users'].name].drop_duplicates().values, # NEW\n",
    "        n_test_users = holdout_valid[data_index['users'].name].nunique() # NEW\n",
    "    )\n",
    "\n",
    "    return training, testset_valid, holdout_valid, testset, holdout, data_description, data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 242627 invalid observations.\n"
     ]
    }
   ],
   "source": [
    "training, testset_valid, holdout_valid, testset, holdout, data_description, data_index = full_preproccessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJOfzHI5dSaz"
   },
   "source": [
    "## Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:11:30.579128Z",
     "start_time": "2022-03-24T12:11:30.564106Z"
    },
    "id": "nXnDXyWrdSbM"
   },
   "outputs": [],
   "source": [
    "def model_evaluate(recommended_items, holdout, holdout_description, alpha=3, topn=10, dcg=False):\n",
    "    itemid = holdout_description['items']\n",
    "    rateid = holdout_description['feedback']\n",
    "    alpha = 3 if holdout_description[\"n_ratings\"] == 5 else 6\n",
    "    n_test_users = recommended_items.shape[0]\n",
    "    holdout_items = holdout[itemid].values\n",
    "    assert recommended_items.shape[0] == len(holdout_items)\n",
    "    \n",
    "    hits_mask = recommended_items[:, :topn] == holdout_items.reshape(-1, 1)\n",
    "    pos_mask = (holdout[rateid] >= alpha).values\n",
    "    neg_mask = (holdout[rateid] < alpha).values\n",
    "    \n",
    "    # HR calculation\n",
    "    #hr = np.sum(hits_mask.any(axis=1)) / n_test_users\n",
    "    hr_pos = np.sum(hits_mask[pos_mask].any(axis=1)) / n_test_users\n",
    "    hr_neg = np.sum(hits_mask[neg_mask].any(axis=1)) / n_test_users\n",
    "    hr = hr_pos + hr_neg\n",
    "    \n",
    "    # MRR calculation\n",
    "    hit_rank = np.where(hits_mask)[1] + 1.0\n",
    "    mrr = np.sum(1 / hit_rank) / n_test_users\n",
    "    pos_hit_rank = np.where(hits_mask[pos_mask])[1] + 1.0\n",
    "    mrr_pos = np.sum(1 / pos_hit_rank) / n_test_users\n",
    "    neg_hit_rank = np.where(hits_mask[neg_mask])[1] + 1.0\n",
    "    mrr_neg = np.sum(1 / neg_hit_rank) / n_test_users\n",
    "    \n",
    "    # Matthews correlation\n",
    "    TP = np.sum(hits_mask[pos_mask]) # + \n",
    "    FP = np.sum(hits_mask[neg_mask]) # +\n",
    "    cond = (hits_mask.sum(axis = 1) == 0)\n",
    "    FN = np.sum(cond[pos_mask])\n",
    "    TN = np.sum(cond[neg_mask])\n",
    "    N = TP+FP+TN+FN\n",
    "    S = (TP+FN)/N\n",
    "    P = (TP+FP)/N\n",
    "    C = (TP/N - S*P) / np.sqrt(P*S*(1-P)*(1-S))\n",
    "    \n",
    "    # DCG calculation\n",
    "    if dcg:\n",
    "        pos_hit_rank = np.where(hits_mask[pos_mask])[1] + 1.0\n",
    "        neg_hit_rank = np.where(hits_mask[neg_mask])[1] + 1.0\n",
    "        ndcg = np.mean(1 / np.log2(pos_hit_rank+1))\n",
    "        ndcl = np.mean(1 / np.log2(neg_hit_rank+1))\n",
    "    \n",
    "    # coverage calculation\n",
    "    n_items = holdout_description['n_items']\n",
    "    cov = np.unique(recommended_items).size / n_items\n",
    "    if dcg:\n",
    "        return hr, hr_pos, hr_neg, mrr, mrr_pos, mrr_neg, cov, C, ndcg, ndcl\n",
    "    else:\n",
    "        return hr, hr_pos, hr_neg, mrr, mrr_pos, mrr_neg, cov, C\n",
    "\n",
    "def make_prediction(tf_scores, holdout, data_description, mode, context=\"\", print_mode=True):\n",
    "    if (mode and print_mode):\n",
    "        print(f\"for context {context} evaluation ({mode}): \\n\")\n",
    "    for n in [5, 10, 20]:\n",
    "        tf_recs = topn_recommendations(tf_scores, n)\n",
    "        hr, hr_pos, hr_neg, mrr, mrr_pos, mrr_neg, cov, C = model_evaluate(tf_recs, holdout, data_description, topn=n)\n",
    "        if (print_mode):\n",
    "            print(f\"HR@{n} = {hr:.4f}, MRR@{n} = {mrr:.4f}, Coverage@{n} = {cov:.4f}\")\n",
    "            print(f\"HR_pos@{n} = {hr_pos:.4f}, HR_neg@{n} = {hr_neg:.4f}\")\n",
    "            print(f\"MRR_pos@{n} = {mrr_pos:.4f}, MRR_neg@{n} = {mrr_neg:.4f}\")\n",
    "            print(f\"Matthews@{n} = {C:.4f}\")\n",
    "            print(\"-------------------------------------\")\n",
    "        if (n == 10):\n",
    "            mrr10 = mrr\n",
    "            hr10 = hr\n",
    "            c10 = C\n",
    "    return mrr10, hr10, c10\n",
    "\n",
    "def valid_mlrank(mlrank):\n",
    "    '''\n",
    "    Only allow ranks that are suitable for truncated SVD computations\n",
    "    on unfolded compressed tensor (the result of ttm product in HOOI).\n",
    "    '''\n",
    "    #s, r1, r2, r3 = mlrank\n",
    "    s, r1, r3 = mlrank\n",
    "    r2 = r1\n",
    "    #print(s, r1, r2, r3)\n",
    "    return r1*r2 > r3 and r1*r3 > r2 and r2*r3 > r1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoFFee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "from scipy.special import softmax\n",
    "\n",
    "def tf_model_build(config, data, data_description, testset, holdout, attention_matrix=np.array([])):\n",
    "    userid = data_description[\"users\"]\n",
    "    itemid = data_description[\"items\"]\n",
    "    feedback = data_description[\"feedback\"]\n",
    "\n",
    "    idx = data[[userid, itemid, feedback]].values\n",
    "    idx[:, -1] = idx[:, -1] - data_description['min_rating'] # works only for integer ratings!\n",
    "    val = np.ones(idx.shape[0], dtype='f8')\n",
    "    \n",
    "    n_users = data_description[\"n_users\"]\n",
    "    n_items = data_description[\"n_items\"]\n",
    "    n_ratings = data_description[\"n_ratings\"]\n",
    "    shape = (n_users, n_items, n_ratings)\n",
    "    core_shape = config['mlrank']\n",
    "    num_iters = config[\"num_iters\"]\n",
    "    \n",
    "    if (attention_matrix.shape[0] == 0):\n",
    "        attention_matrix = form_attention_matrix(\n",
    "            data_description['n_ratings'],\n",
    "            **config['params'],\n",
    "            format = 'csr'\n",
    "        )\n",
    "        \n",
    "    attention_matrix = np.array(attention_matrix)\n",
    "\n",
    "    item_popularity = (\n",
    "        data[itemid]\n",
    "        .value_counts(sort=False)\n",
    "        .reindex(range(n_items))\n",
    "        .fillna(1)\n",
    "        .values\n",
    "    )\n",
    "    scaling_weights = get_scaling_weights(item_popularity, scaling=config[\"scaling\"])\n",
    "\n",
    "    with io.capture_output() as captured:\n",
    "        u0, u1, u2 = sa_hooi(\n",
    "            idx, val, shape, config[\"mlrank\"],\n",
    "            attention_matrix = attention_matrix,\n",
    "            scaling_weights = scaling_weights,\n",
    "            testset = testset,\n",
    "            holdout = holdout,\n",
    "            data_description = data_description,\n",
    "            max_iters = config[\"num_iters\"],\n",
    "            parallel_ttm = True,\n",
    "            randomized = config[\"randomized\"],\n",
    "            growth_tol = config[\"growth_tol\"],\n",
    "            seed = config[\"seed\"],\n",
    "            iter_callback = None,\n",
    "        )\n",
    "    \n",
    "    return u0, u1, u2, attention_matrix    \n",
    "    \n",
    "config = {\n",
    "    \"scaling\": 1,\n",
    "    \"mlrank\": (30, 30, data_description['n_ratings']),\n",
    "    \"n_ratings\": data_description['n_ratings'],\n",
    "    \"num_iters\": 5,\n",
    "    \"params\": None,\n",
    "    \"randomized\": True,\n",
    "    \"growth_tol\": 1e-4,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "def tf_scoring(params, data, data_description, context=[\"3+4+5\"]):\n",
    "    user_factors, item_factors, feedback_factors, attention_matrix = params\n",
    "    userid = data_description[\"users\"]\n",
    "    itemid = data_description[\"items\"]\n",
    "    feedback = data_description[\"feedback\"]\n",
    "\n",
    "    data = data.sort_values(userid)\n",
    "    useridx = data[userid]\n",
    "    itemidx = data[itemid].values\n",
    "    ratings = data[feedback].values\n",
    "    ratings = ratings - data_description['min_rating'] # NEW\n",
    "    \n",
    "    n_users = useridx.nunique()\n",
    "    n_items = data_description['n_items']\n",
    "    n_ratings = data_description['n_ratings']\n",
    "    \n",
    "    inv_attention = solve_triangular(attention_matrix, np.eye(n_ratings), lower=True)\n",
    "    \n",
    "    tensor_outer = tensor_outer_at('cpu')\n",
    "    #matrix_softmax = softmax(inv_attention.T @ feedback_factors)\n",
    "    matrix_softmax = inv_attention.T @ feedback_factors\n",
    "    #\n",
    "    if (n_ratings == 10):\n",
    "        coef = 2\n",
    "    else:\n",
    "        coef = 1\n",
    "        \n",
    "    if (context == \"5\"): # make softmax \n",
    "        inv_aT_feedback = matrix_softmax[(-1 * coef) , :]\n",
    "    elif (context == \"4+5\"):\n",
    "        inv_aT_feedback = np.sum(matrix_softmax[(-2 * coef):, :], axis=0)\n",
    "    elif (context == \"3+4+5\"):\n",
    "        inv_aT_feedback = np.sum(matrix_softmax[(-3 * coef):, :], axis=0)\n",
    "    #elif (context == \"2+3+4+5\"):\n",
    "    #    inv_aT_feedback = np.sum(matrix_softmax[-4:, :], axis=0)\n",
    "    elif (context == \"3+4+5-2-1\"):\n",
    "        inv_aT_feedback = np.sum(matrix_softmax[(-3 * coef):, :], axis=0) - np.sum(matrix_softmax[:(2 * coef), :], axis=0)\n",
    "        \n",
    "    scores = tensor_outer(\n",
    "        1.0,\n",
    "        item_factors,\n",
    "        attention_matrix @ feedback_factors,\n",
    "        itemidx,\n",
    "        ratings\n",
    "    )\n",
    "    scores = np.add.reduceat(scores, np.r_[0, np.where(np.diff(useridx))[0]+1]) # sort by users\n",
    "    scores = np.tensordot(\n",
    "        scores,\n",
    "        inv_aT_feedback,\n",
    "        axes=(2, 0)\n",
    "    ).dot(item_factors.T)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "from polara.evaluation.pipelines import random_grid\n",
    "\n",
    "def full_pipeline(config, training, data_description, testset_valid, holdout_valid, testset, holdout, attention_matrix):\n",
    "\n",
    "    config[\"mlrank\"] = (64, 64, data_description[\"n_ratings\"])\n",
    "    print(\"Starting pipeline...\")\n",
    "    print(\"Training with different context in progress...\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "\n",
    "    for context in [\"5\", \"4+5\", \"3+4+5\", \"3+4+5-2-1\"]:\n",
    "        tf_params = tf_model_build(config, training, data_description, testset_valid, holdout_valid, attention_matrix=attention_matrix)\n",
    "        seen_data = testset_valid\n",
    "        tf_scores = tf_scoring(tf_params, seen_data, data_description, context)\n",
    "        downvote_seen_items(tf_scores, seen_data, data_description)\n",
    "        cur_mrr, cur_hr, cur_C = make_prediction(tf_scores, holdout_valid, data_description, \"Validation\", context)\n",
    "        print(\"------------------------------------------------------\")\n",
    "\n",
    "    print(f\"Tuning model for all contexts...\\n\")\n",
    "\n",
    "    rank_grid = []\n",
    "    for i in range(5, 9):\n",
    "        rank_grid.append(2 * 2 ** i)\n",
    "        rank_grid.append(3 * 2 ** i)\n",
    "    \n",
    "    rank_grid = np.array(rank_grid)\n",
    "    tf_hyper = {\n",
    "    'scaling': [0.6], #np.linspace(0, 2, 21),\n",
    "    'r1': rank_grid, #np.arange(100, 220, 25),\n",
    "    #'r2': np.arange(50, 801, 25),\n",
    "    'r3': range(2, 11, 2), # change\n",
    "    }\n",
    "\n",
    "    grid, param_names = random_grid(tf_hyper, n=0)\n",
    "    tf_grid = [tuple(mlrank) for mlrank in grid if valid_mlrank(mlrank)]\n",
    "\n",
    "    hr_tf = {}\n",
    "    hr_pos_tf = {}\n",
    "    hr_neg_tf = {}\n",
    "    mrr_tf = {}\n",
    "    mrr_pos_tf = {}\n",
    "    mrr_neg_tf = {}\n",
    "    cov_tf = {}\n",
    "    C_tf = {}\n",
    "    \n",
    "    seen_data = testset_valid\n",
    "    \n",
    "    for mlrank in tqdm(tf_grid):\n",
    "        with io.capture_output() as captured:\n",
    "            r1, r3 = mlrank[1:]\n",
    "            cur_mlrank = tuple((r1, r1, r3))\n",
    "            config['mlrank'] = cur_mlrank\n",
    "            config['scaling'] = mlrank[0]\n",
    "            tf_params = tf_model_build(config, training, data_description, testset_valid, holdout_valid, attention_matrix=attention_matrix)\n",
    "            for context in [\"5\", \"4+5\", \"3+4+5\", \"3+4+5-2-1\"]:\n",
    "                tf_scores = tf_scoring(tf_params, seen_data, data_description, context)\n",
    "                downvote_seen_items(tf_scores, seen_data, data_description)\n",
    "                tf_recs = topn_recommendations(tf_scores, topn=10)\n",
    "                \n",
    "                hr, hr_pos, hr_neg, mrr, mrr_pos, mrr_neg, cov, C = model_evaluate(tf_recs, holdout_valid, data_description, topn=10)\n",
    "                hr_tf[(context, cur_mlrank, mlrank[0])] = hr\n",
    "                hr_pos_tf[(context, cur_mlrank, mlrank[0])] = hr_pos\n",
    "                hr_neg_tf[(context, cur_mlrank, mlrank[0])] = hr_neg\n",
    "                mrr_tf[(context, cur_mlrank, mlrank[0])] = mrr\n",
    "                mrr_pos_tf[(context, cur_mlrank, mlrank[0])] = mrr_pos\n",
    "                mrr_neg_tf[(context, cur_mlrank, mlrank[0])] = mrr_neg\n",
    "                cov_tf[(context, cur_mlrank, mlrank[0])] = cov\n",
    "                C_tf[(context, cur_mlrank, mlrank[0])] = C\n",
    "\n",
    "    print(f'Best HR={pd.Series(hr_tf).max():.4f} achieved with context {pd.Series(hr_tf).idxmax()[0]} and mlrank = {pd.Series(hr_tf).idxmax()[1]} and scale factor = {pd.Series(hr_tf).idxmax()[2]}')\n",
    "    print(f'Best HR_pos={pd.Series(hr_pos_tf).max():.4f} achieved with context {pd.Series(hr_pos_tf).idxmax()[0]} and mlrank = {pd.Series(hr_pos_tf).idxmax()[1]} and scale factor = {pd.Series(hr_pos_tf).idxmax()[2]}')\n",
    "    print(f'Best HR_neg={pd.Series(hr_neg_tf).min():.4f} achieved with context {pd.Series(hr_neg_tf).idxmin()[0]} and mlrank = {pd.Series(hr_neg_tf).idxmin()[1]} and scale factor = {pd.Series(hr_neg_tf).idxmin()[2]}')\n",
    "    \n",
    "    print(f'Best MRR={pd.Series(mrr_tf).max():.4f} achieved with context {pd.Series(mrr_tf).idxmax()[0]} and mlrank = {pd.Series(mrr_tf).idxmax()[1]} and scale factor = {pd.Series(mrr_tf).idxmax()[2]}')\n",
    "    print(f'Best MRR_pos={pd.Series(mrr_pos_tf).max():.4f} achieved with context {pd.Series(mrr_pos_tf).idxmax()[0]} and mlrank = {pd.Series(mrr_pos_tf).idxmax()[1]} and scale factor = {pd.Series(mrr_pos_tf).idxmax()[2]}')\n",
    "    print(f'Best MRR_neg={pd.Series(mrr_neg_tf).min():.4f} achieved with context {pd.Series(mrr_neg_tf).idxmin()[0]} and mlrank = {pd.Series(mrr_neg_tf).idxmin()[1]} and scale factor = {pd.Series(mrr_neg_tf).idxmin()[2]}')\n",
    "    \n",
    "    print(f'Best Matthews={pd.Series(C_tf).max():.4f} achieved with context {pd.Series(C_tf).idxmax()[0]} and mlrank = {pd.Series(C_tf).idxmax()[1]} and scale factor = {pd.Series(C_tf).idxmax()[2]}')\n",
    "                          \n",
    "    print(f'COV={pd.Series(cov_tf)[pd.Series(C_tf).idxmax()]:.4f} (based on best Matthews value)')\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    print(\"Evaluation of the best model on test holdout in progress...\\n\")\n",
    "    \n",
    "    print(\"Best by MRR@10:\\n\")\n",
    "    config[\"mlrank\"] = pd.Series(mrr_pos_tf).idxmax()[1]\n",
    "    tf_params = tf_model_build(config, training, data_description, testset, holdout, attention_matrix=attention_matrix)\n",
    "\n",
    "    seen_data = testset\n",
    "    tf_scores = tf_scoring(tf_params, seen_data, data_description, pd.Series(mrr_pos_tf).idxmax()[0])\n",
    "    downvote_seen_items(tf_scores, seen_data, data_description)\n",
    "    cur_mrr, cur_hr, cur_C = make_prediction(tf_scores, holdout, data_description, \"Test\", pd.Series(mrr_pos_tf).idxmax()[0])\n",
    "    \n",
    "    print(\"---------------------------------------------------------\")\n",
    "    \n",
    "    print(\"Best by HR@10:\\n\")\n",
    "    config[\"mlrank\"] = pd.Series(hr_pos_tf).idxmax()[1]\n",
    "    tf_params = tf_model_build(config, training, data_description, testset, holdout, attention_matrix=attention_matrix)\n",
    "\n",
    "    seen_data = testset\n",
    "    tf_scores = tf_scoring(tf_params, seen_data, data_description, pd.Series(hr_pos_tf).idxmax()[0])\n",
    "    downvote_seen_items(tf_scores, seen_data, data_description)\n",
    "    cur_mrr, cur_hr, cur_C = make_prediction(tf_scores, holdout, data_description, \"Test\", pd.Series(hr_pos_tf).idxmax()[0])\n",
    "    \n",
    "    print(\"---------------------------------------------------------\")\n",
    "    \n",
    "    print(\"Best by Matthews@10:\\n\")\n",
    "    config[\"mlrank\"] = pd.Series(C_tf).idxmax()[1]\n",
    "    tf_params = tf_model_build(config, training, data_description, testset, holdout, attention_matrix=attention_matrix)\n",
    "\n",
    "    seen_data = testset\n",
    "    tf_scores = tf_scoring(tf_params, seen_data, data_description, pd.Series(C_tf).idxmax()[0])\n",
    "    downvote_seen_items(tf_scores, seen_data, data_description)\n",
    "    cur_mrr, cur_hr, cur_C = make_prediction(tf_scores, holdout, data_description, \"Test\", pd.Series(C_tf).idxmax()[0])\n",
    "    print(\"Pipeline ended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline...\n",
      "Training with different context in progress...\n",
      "------------------------------------------------------\n",
      "for context 5 evaluation (Validation): \n",
      "\n",
      "HR@5 = 0.0342, MRR@5 = 0.0182, Coverage@5 = 0.0843\n",
      "HR_pos@5 = 0.0325, HR_neg@5 = 0.0017\n",
      "MRR_pos@5 = 0.0173, MRR_neg@5 = 0.0009\n",
      "Matthews@5 = 0.0532\n",
      "-------------------------------------\n",
      "HR@10 = 0.0548, MRR@10 = 0.0209, Coverage@10 = 0.1075\n",
      "HR_pos@10 = 0.0518, HR_neg@10 = 0.0030\n",
      "MRR_pos@10 = 0.0198, MRR_neg@10 = 0.0011\n",
      "Matthews@10 = 0.0652\n",
      "-------------------------------------\n",
      "HR@20 = 0.0872, MRR@20 = 0.0231, Coverage@20 = 0.1399\n",
      "HR_pos@20 = 0.0816, HR_neg@20 = 0.0056\n",
      "MRR_pos@20 = 0.0218, MRR_neg@20 = 0.0013\n",
      "Matthews@20 = 0.0760\n",
      "-------------------------------------\n",
      "------------------------------------------------------\n",
      "for context 4+5 evaluation (Validation): \n",
      "\n",
      "HR@5 = 0.0492, MRR@5 = 0.0264, Coverage@5 = 0.0784\n",
      "HR_pos@5 = 0.0474, HR_neg@5 = 0.0018\n",
      "MRR_pos@5 = 0.0255, MRR_neg@5 = 0.0009\n",
      "Matthews@5 = 0.0731\n",
      "-------------------------------------\n",
      "HR@10 = 0.0761, MRR@10 = 0.0298, Coverage@10 = 0.0991\n",
      "HR_pos@10 = 0.0728, HR_neg@10 = 0.0033\n",
      "MRR_pos@10 = 0.0288, MRR_neg@10 = 0.0010\n",
      "Matthews@10 = 0.0868\n",
      "-------------------------------------\n",
      "HR@20 = 0.1179, MRR@20 = 0.0327, Coverage@20 = 0.1287\n",
      "HR_pos@20 = 0.1117, HR_neg@20 = 0.0062\n",
      "MRR_pos@20 = 0.0314, MRR_neg@20 = 0.0012\n",
      "Matthews@20 = 0.1010\n",
      "-------------------------------------\n",
      "------------------------------------------------------\n",
      "for context 3+4+5 evaluation (Validation): \n",
      "\n",
      "HR@5 = 0.0473, MRR@5 = 0.0266, Coverage@5 = 0.0753\n",
      "HR_pos@5 = 0.0451, HR_neg@5 = 0.0022\n",
      "MRR_pos@5 = 0.0256, MRR_neg@5 = 0.0010\n",
      "Matthews@5 = 0.0657\n",
      "-------------------------------------\n",
      "HR@10 = 0.0756, MRR@10 = 0.0303, Coverage@10 = 0.0964\n",
      "HR_pos@10 = 0.0715, HR_neg@10 = 0.0041\n",
      "MRR_pos@10 = 0.0290, MRR_neg@10 = 0.0012\n",
      "Matthews@10 = 0.0784\n",
      "-------------------------------------\n",
      "HR@20 = 0.1189, MRR@20 = 0.0332, Coverage@20 = 0.1258\n",
      "HR_pos@20 = 0.1108, HR_neg@20 = 0.0080\n",
      "MRR_pos@20 = 0.0317, MRR_neg@20 = 0.0015\n",
      "Matthews@20 = 0.0864\n",
      "-------------------------------------\n",
      "------------------------------------------------------\n",
      "for context 3+4+5-2-1 evaluation (Validation): \n",
      "\n",
      "HR@5 = 0.0464, MRR@5 = 0.0263, Coverage@5 = 0.0748\n",
      "HR_pos@5 = 0.0446, HR_neg@5 = 0.0019\n",
      "MRR_pos@5 = 0.0254, MRR_neg@5 = 0.0009\n",
      "Matthews@5 = 0.0686\n",
      "-------------------------------------\n",
      "HR@10 = 0.0739, MRR@10 = 0.0299, Coverage@10 = 0.0978\n",
      "HR_pos@10 = 0.0704, HR_neg@10 = 0.0035\n",
      "MRR_pos@10 = 0.0288, MRR_neg@10 = 0.0011\n",
      "Matthews@10 = 0.0821\n",
      "-------------------------------------\n",
      "HR@20 = 0.1162, MRR@20 = 0.0327, Coverage@20 = 0.1277\n",
      "HR_pos@20 = 0.1096, HR_neg@20 = 0.0067\n",
      "MRR_pos@20 = 0.0314, MRR_neg@20 = 0.0013\n",
      "Matthews@20 = 0.0955\n",
      "-------------------------------------\n",
      "------------------------------------------------------\n",
      "Tuning model for all contexts...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [07:26<1:37:59, 158.91s/it]"
     ]
    }
   ],
   "source": [
    "attention_matrix = np.eye(10)\n",
    "full_pipeline(config, training, data_description, testset_valid, holdout_valid, testset, holdout, attention_matrix=attention_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EW7SFRIqqle7"
   },
   "source": [
    "# Random Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:11:44.551668Z",
     "start_time": "2022-03-24T12:11:44.536651Z"
    },
    "id": "KMEOnT4sqoTq"
   },
   "outputs": [],
   "source": [
    "def build_random_model(trainset, trainset_description):\n",
    "    itemid = trainset_description['items']\n",
    "    n_items = trainset[itemid].max() + 1\n",
    "    random_state = np.random.RandomState(42)\n",
    "    return n_items, random_state\n",
    "\n",
    "def random_model_scoring(params, testset, testset_description):\n",
    "    n_items, random_state = params\n",
    "    n_users = testset_description['n_test_users']\n",
    "    scores = random_state.rand(n_users, n_items)\n",
    "    return scores\n",
    "\n",
    "def simple_model_recom_func(scores, topn=20):\n",
    "    recommendations = np.apply_along_axis(topidx, 1, scores, topn)\n",
    "    return recommendations\n",
    "\n",
    "def topidx(a, topn):\n",
    "    parted = np.argpartition(a, -topn)[-topn:]\n",
    "    return parted[np.argsort(-a[parted])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQaaMX1erOcc"
   },
   "source": [
    "# Popularity-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:11:45.116079Z",
     "start_time": "2022-03-24T12:11:45.108082Z"
    },
    "id": "oI3UZ4KMrR4i"
   },
   "outputs": [],
   "source": [
    "def build_popularity_model(trainset, trainset_description):\n",
    "    itemid = trainset_description['items']\n",
    "    item_popularity = trainset[itemid].value_counts()\n",
    "    return item_popularity\n",
    "\n",
    "def popularity_model_scoring(params, testset, testset_description):\n",
    "    item_popularity = params\n",
    "    n_items = item_popularity.index.max() + 1\n",
    "    n_users = testset_description['n_test_users']\n",
    "    # fill in popularity scores for each item with indices from 0 to n_items-1\n",
    "    popularity_scores = np.zeros(n_items,)\n",
    "    popularity_scores[item_popularity.index] = item_popularity.values\n",
    "    # same scores for each test user\n",
    "    scores = np.tile(popularity_scores, n_users).reshape(n_users, n_items)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-c7NJyjoxbVF"
   },
   "source": [
    "# PureSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:11:46.267532Z",
     "start_time": "2022-03-24T12:11:46.254529Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHfx8WVYvJsQ",
    "outputId": "8791e807-11c9-41a1-c3a9-7b68cf85822d"
   },
   "outputs": [],
   "source": [
    "def matrix_from_observations(data, data_description):\n",
    "    useridx = data[data_description['users']]\n",
    "    itemidx = data[data_description['items']]\n",
    "    values = data[data_description['feedback']]\n",
    "    return csr_matrix((values, (useridx, itemidx)), dtype='f8')\n",
    "\n",
    "\n",
    "def build_svd_model(config, data, data_description):\n",
    "    source_matrix = matrix_from_observations(data, data_description)\n",
    "    D = norm(source_matrix, axis=0)\n",
    "    A = source_matrix.dot(diags(D**(config['f']-1)))\n",
    "    _, _, vt = svds(A, k=config['rank'], return_singular_vectors='vh')\n",
    "#     singular_values = s[::-1]\n",
    "    item_factors = np.ascontiguousarray(vt[::-1, :].T)\n",
    "    return item_factors\n",
    "\n",
    "def svd_model_scoring(params, data, data_description):\n",
    "    item_factors = params\n",
    "    test_data = data.assign(\n",
    "        userid = pd.factorize(data['userid'])[0]\n",
    "    )\n",
    "    test_matrix = matrix_from_observations(test_data, data_description)\n",
    "    scores = test_matrix.dot(item_factors) @ item_factors.T\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkKi34mwyWUo"
   },
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T10:15:25.329453Z",
     "start_time": "2022-03-24T10:15:25.314464Z"
    }
   },
   "outputs": [],
   "source": [
    "rank_grid = []\n",
    "for i in range(5, 10):\n",
    "    rank_grid.append(2 * 2 ** i)\n",
    "    rank_grid.append(3 * 2 ** i)\n",
    "    \n",
    "rank_grid = np.array(rank_grid)\n",
    "\n",
    "f_grid = np.linspace(0, 2, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T10:38:55.267179Z",
     "start_time": "2022-03-24T10:18:22.838941Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [2:14:13<00:00, 38.35s/it]  \n"
     ]
    }
   ],
   "source": [
    "hr_tf = {}\n",
    "mrr_tf = {}\n",
    "C_tf = {}\n",
    "grid = list(zip(np.meshgrid(rank_grid, f_grid)[0].flatten(), np.meshgrid(rank_grid, f_grid)[1].flatten()))\n",
    "for params in tqdm(grid):\n",
    "    r, f = params\n",
    "    svd_config = {'rank': int(r), 'f': f}\n",
    "    svd_params = build_svd_model(svd_config, training, data_description)\n",
    "    svd_scores = svd_model_scoring(svd_params, testset_valid, data_description)\n",
    "    downvote_seen_items(svd_scores, testset_valid, data_description)\n",
    "    svd_recs = topn_recommendations(svd_scores, topn=10)\n",
    "    hr, hr_pos, hr_neg, mrr, mrr_pos, mrr_neg, cov, C = model_evaluate(svd_recs, holdout_valid, data_description, alpha=3, topn=10, dcg=False)\n",
    "    hr_tf[f'r={r}, f={f:.2f}'] = hr\n",
    "    mrr_tf[f'r={r}, f={f:.2f}'] = mrr\n",
    "    C_tf[f'r={r}, f={f:.2f}'] = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T10:40:21.195015Z",
     "start_time": "2022-03-24T10:40:21.183996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r=1024, f=0.30 0.10775473399458971\n",
      "r=1536, f=0.20 0.10767959122332431\n",
      "r=768, f=0.30 0.1073038773669973\n",
      "r=1024, f=0.20 0.10617673579801623\n",
      "r=512, f=0.30 0.10595130748422\n",
      "r=256, f=0.40 0.10489930868650436\n",
      "r=512, f=0.40 0.10459873760144275\n",
      "r=1536, f=0.10 0.10422302374511572\n",
      "r=768, f=0.20 0.1040727382025849\n",
      "r=1536, f=0.30 0.10392245266005411\n"
     ]
    }
   ],
   "source": [
    "hr_sorted = sorted(hr_tf, key=hr_tf.get, reverse=True)\n",
    "for i in range(10):\n",
    "    print(hr_sorted[i], hr_tf[hr_sorted[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T10:43:48.278788Z",
     "start_time": "2022-03-24T10:43:48.266777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r=1024, f=0.30 0.04348428681160872\n",
      "r=512, f=0.40 0.04280507965133754\n",
      "r=768, f=0.30 0.042479580250094226\n",
      "r=1536, f=0.20 0.04247454091344984\n",
      "r=1536, f=0.30 0.04241356195819676\n",
      "r=768, f=0.40 0.04206563899981393\n",
      "r=384, f=0.40 0.04103534811379716\n",
      "r=512, f=0.30 0.041015131130099566\n",
      "r=1024, f=0.20 0.040927464563623256\n",
      "r=1024, f=0.40 0.04079131301854476\n"
     ]
    }
   ],
   "source": [
    "mrr_sorted = sorted(mrr_tf, key=mrr_tf.get, reverse=True)\n",
    "for i in range(10):\n",
    "    print(mrr_sorted[i], mrr_tf[mrr_sorted[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T10:40:42.540415Z",
     "start_time": "2022-03-24T10:40:42.522418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r=96, f=0.60 0.09007364680788844\n",
      "r=384, f=0.30 0.08937860020678351\n",
      "r=96, f=0.50 0.0893574990370936\n",
      "r=96, f=0.70 0.08907998278107779\n",
      "r=256, f=0.30 0.08902904226489296\n",
      "r=256, f=0.40 0.08898962117182473\n",
      "r=192, f=0.40 0.08886710000112372\n",
      "r=384, f=0.20 0.08850397806585301\n",
      "r=64, f=0.80 0.0883435454046812\n",
      "r=64, f=0.70 0.08743728377081639\n"
     ]
    }
   ],
   "source": [
    "C_sorted = sorted(C_tf, key=C_tf.get, reverse=True)\n",
    "for i in range(10):\n",
    "    print(C_sorted[i], C_tf[C_sorted[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:28:23.961834Z",
     "start_time": "2022-03-24T12:28:23.944826Z"
    }
   },
   "outputs": [],
   "source": [
    "data_description = dict(\n",
    "    users = data_index['users'].name,\n",
    "    items = data_index['items'].name,\n",
    "    feedback = 'rating',\n",
    "    n_users = len(data_index['users']),\n",
    "    n_items = len(data_index['items']),\n",
    "    n_ratings = training['rating'].nunique(),\n",
    "    min_rating = training['rating'].min(),\n",
    "    test_users = holdout[data_index['users'].name].drop_duplicates().values,\n",
    "    n_test_users = holdout[data_index['users'].name].nunique()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:28:25.027454Z",
     "start_time": "2022-03-24T12:28:24.845212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context  evaluation (Test): \n",
      "\n",
      "HR@5 = 0.0004, MRR@5 = 0.0001, Coverage@5 = 0.9990\n",
      "HR_pos@5 = 0.0004, HR_neg@5 = 0.0000\n",
      "MRR_pos@5 = 0.0001, MRR_neg@5 = 0.0000\n",
      "Matthews@5 = 0.0077\n",
      "-------------------------------------\n",
      "HR@10 = 0.0010, MRR@10 = 0.0002, Coverage@10 = 1.0000\n",
      "HR_pos@10 = 0.0010, HR_neg@10 = 0.0000\n",
      "MRR_pos@10 = 0.0002, MRR_neg@10 = 0.0000\n",
      "Matthews@10 = 0.0129\n",
      "-------------------------------------\n",
      "HR@20 = 0.0025, MRR@20 = 0.0003, Coverage@20 = 1.0000\n",
      "HR_pos@20 = 0.0024, HR_neg@20 = 0.0001\n",
      "MRR_pos@20 = 0.0003, MRR_neg@20 = 0.0000\n",
      "Matthews@20 = 0.0155\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rnd_params = build_random_model(training, data_description)\n",
    "rnd_scores = random_model_scoring(rnd_params, None, data_description)\n",
    "downvote_seen_items(rnd_scores, testset, data_description)\n",
    "\n",
    "_ = make_prediction(rnd_scores, holdout, data_description, mode=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:28:33.944233Z",
     "start_time": "2022-03-24T12:28:33.910186Z"
    }
   },
   "outputs": [],
   "source": [
    "pop_params = build_popularity_model(training, data_description)\n",
    "pop_scores = popularity_model_scoring(pop_params, None, data_description)\n",
    "downvote_seen_items(pop_scores, testset, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:28:34.431536Z",
     "start_time": "2022-03-24T12:28:34.320513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context  evaluation (Test): \n",
      "\n",
      "HR@5 = 0.0272, MRR@5 = 0.0149, Coverage@5 = 0.0177\n",
      "HR_pos@5 = 0.0248, HR_neg@5 = 0.0025\n",
      "MRR_pos@5 = 0.0136, MRR_neg@5 = 0.0013\n",
      "Matthews@5 = 0.0230\n",
      "-------------------------------------\n",
      "HR@10 = 0.0475, MRR@10 = 0.0176, Coverage@10 = 0.0276\n",
      "HR_pos@10 = 0.0425, HR_neg@10 = 0.0050\n",
      "MRR_pos@10 = 0.0160, MRR_neg@10 = 0.0016\n",
      "Matthews@10 = 0.0212\n",
      "-------------------------------------\n",
      "HR@20 = 0.0747, MRR@20 = 0.0194, Coverage@20 = 0.0423\n",
      "HR_pos@20 = 0.0662, HR_neg@20 = 0.0084\n",
      "MRR_pos@20 = 0.0176, MRR_neg@20 = 0.0019\n",
      "Matthews@20 = 0.0206\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "_ = make_prediction(pop_scores, holdout, data_description, mode=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PureSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:28:39.191316Z",
     "start_time": "2022-03-24T12:28:36.704692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'rank': 1024, 'f': 0.3}, 'HR')\n",
      "for context  evaluation (Test): \n",
      "\n",
      "HR@5 = 0.0676, MRR@5 = 0.0380, Coverage@5 = 0.2482\n",
      "HR_pos@5 = 0.0638, HR_neg@5 = 0.0039\n",
      "MRR_pos@5 = 0.0360, MRR_neg@5 = 0.0020\n",
      "Matthews@5 = 0.0629\n",
      "-------------------------------------\n",
      "HR@10 = 0.1078, MRR@10 = 0.0432, Coverage@10 = 0.3311\n",
      "HR_pos@10 = 0.1003, HR_neg@10 = 0.0075\n",
      "MRR_pos@10 = 0.0408, MRR_neg@10 = 0.0024\n",
      "Matthews@10 = 0.0692\n",
      "-------------------------------------\n",
      "HR@20 = 0.1599, MRR@20 = 0.0467, Coverage@20 = 0.4363\n",
      "HR_pos@20 = 0.1481, HR_neg@20 = 0.0118\n",
      "MRR_pos@20 = 0.0440, MRR_neg@20 = 0.0027\n",
      "Matthews@20 = 0.0812\n",
      "-------------------------------------\n",
      "({'rank': 1024, 'f': 0.3}, 'MRR')\n",
      "for context  evaluation (Test): \n",
      "\n",
      "HR@5 = 0.0676, MRR@5 = 0.0380, Coverage@5 = 0.2482\n",
      "HR_pos@5 = 0.0638, HR_neg@5 = 0.0039\n",
      "MRR_pos@5 = 0.0360, MRR_neg@5 = 0.0020\n",
      "Matthews@5 = 0.0629\n",
      "-------------------------------------\n",
      "HR@10 = 0.1078, MRR@10 = 0.0432, Coverage@10 = 0.3311\n",
      "HR_pos@10 = 0.1003, HR_neg@10 = 0.0075\n",
      "MRR_pos@10 = 0.0408, MRR_neg@10 = 0.0024\n",
      "Matthews@10 = 0.0692\n",
      "-------------------------------------\n",
      "HR@20 = 0.1599, MRR@20 = 0.0467, Coverage@20 = 0.4363\n",
      "HR_pos@20 = 0.1481, HR_neg@20 = 0.0118\n",
      "MRR_pos@20 = 0.0440, MRR_neg@20 = 0.0027\n",
      "Matthews@20 = 0.0812\n",
      "-------------------------------------\n",
      "({'rank': 96, 'f': 0.6}, 'MC')\n",
      "for context  evaluation (Test): \n",
      "\n",
      "HR@5 = 0.0616, MRR@5 = 0.0342, Coverage@5 = 0.1268\n",
      "HR_pos@5 = 0.0585, HR_neg@5 = 0.0031\n",
      "MRR_pos@5 = 0.0326, MRR_neg@5 = 0.0016\n",
      "Matthews@5 = 0.0656\n",
      "-------------------------------------\n",
      "HR@10 = 0.0976, MRR@10 = 0.0390, Coverage@10 = 0.1646\n",
      "HR_pos@10 = 0.0920, HR_neg@10 = 0.0056\n",
      "MRR_pos@10 = 0.0370, MRR_neg@10 = 0.0019\n",
      "Matthews@10 = 0.0769\n",
      "-------------------------------------\n",
      "HR@20 = 0.1475, MRR@20 = 0.0424, Coverage@20 = 0.2146\n",
      "HR_pos@20 = 0.1370, HR_neg@20 = 0.0105\n",
      "MRR_pos@20 = 0.0401, MRR_neg@20 = 0.0023\n",
      "Matthews@20 = 0.0804\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for_hr = sorted(hr_tf, key=hr_tf.get, reverse=True)[0]\n",
    "for_mrr = sorted(mrr_tf, key=mrr_tf.get, reverse=True)[0]\n",
    "for_mc = sorted(C_tf, key=C_tf.get, reverse=True)[0]\n",
    "\n",
    "svd_config_hr = {'rank': int(for_hr.split(\",\")[0][2:]), 'f': float(for_hr.split(\",\")[1][3:])}\n",
    "svd_config_mrr = {'rank': int(for_mrr.split(\",\")[0][2:]), 'f': float(for_mrr.split(\",\")[1][3:])}\n",
    "svd_config_mc = {'rank': int(for_mc.split(\",\")[0][2:]), 'f': float(for_mc.split(\",\")[1][3:])}\n",
    "\n",
    "svd_configs = [(svd_config_hr, \"HR\"), (svd_config_mrr, \"MRR\"), (svd_config_mc, \"MC\")]\n",
    "\n",
    "for svd_config in svd_configs:\n",
    "    print(svd_config)\n",
    "    svd_params = build_svd_model(svd_config[0], training, data_description)\n",
    "    svd_scores = svd_model_scoring(svd_params, testset, data_description)\n",
    "    downvote_seen_items(svd_scores, testset, data_description)\n",
    "\n",
    "    _ = make_prediction(svd_scores, holdout, data_description, mode=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CoFFeeProject.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "009c838d92940ae6fa3c0eca0f0908a58be7fe030119f0cd30e204cb459dcff7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.306px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
