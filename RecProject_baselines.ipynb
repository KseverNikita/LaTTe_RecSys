{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVoK_7xHdaa9",
    "outputId": "5068028f-6526-41db-ab84-9989d1f39bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polara\n",
      "  Cloning https://github.com/evfro/polara.git (to revision develop) to /tmp/pip-install-r7bkr9kg/polara_0a26ea2247b748b6b9ee7f29e90e0cf5\n",
      "  Running command git clone -q https://github.com/evfro/polara.git /tmp/pip-install-r7bkr9kg/polara_0a26ea2247b748b6b9ee7f29e90e0cf5\n",
      "  Running command git checkout -b develop --track origin/develop\n",
      "  Switched to a new branch 'develop'\n",
      "  Branch 'develop' set up to track remote branch 'develop' from 'origin'.\n",
      "  Resolved https://github.com/evfro/polara.git to commit 4de4ca7d6f901e32f1e045f190bcb09587162397\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --no-cache-dir --upgrade git+https://github.com/evfro/polara.git@develop#egg=polara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:11:20.346846Z",
     "start_time": "2022-03-24T12:11:18.564058Z"
    },
    "id": "o_HJgdQFdSaf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import polara\n",
    "from polara import get_movielens_data\n",
    "from polara.preprocessing.dataframes import leave_one_out, reindex\n",
    "\n",
    "from dataprep import transform_indices\n",
    "from evaluation import topn_recommendations, model_evaluate, downvote_seen_items\n",
    "\n",
    "from polara.lib.tensor import hooi\n",
    "from polara.lib.sparse import tensor_outer_at\n",
    "from polara.evaluation.pipelines import random_grid\n",
    "\n",
    "from sa_hooi import sa_hooi, form_attention_matrix, get_scaling_weights, generate_position_projector\n",
    "\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "from scipy.sparse.linalg import norm, svds\n",
    "\n",
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NbP1tmBdSas"
   },
   "source": [
    "# Data preproccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_preproccessing():\n",
    "    data = get_movielens_data(include_time=True)\n",
    "    test_timepoint = data['timestamp'].quantile(\n",
    "    q=0.8, interpolation='nearest'\n",
    "    )\n",
    "\n",
    "    test_data_ = data.query('timestamp >= @test_timepoint')\n",
    "    train_data_ = data.query(\n",
    "    'userid not in @test_data_.userid.unique() and timestamp < @test_timepoint'\n",
    "    )\n",
    "    \n",
    "    training, data_index = transform_indices(train_data_.copy(), 'userid', 'movieid')\n",
    "    test_data = reindex(test_data_, data_index['items'])\n",
    "\n",
    "    testset_, holdout_ = leave_one_out(\n",
    "    test_data, target='timestamp', sample_top=True, random_state=0\n",
    "    )\n",
    "    testset_valid_, holdout_valid_ = leave_one_out(\n",
    "        testset_, target='timestamp', sample_top=True, random_state=0\n",
    "    )\n",
    "\n",
    "    test_users_val = np.intersect1d(testset_valid_.userid.unique(), holdout_valid_.userid.unique())\n",
    "    testset_valid = testset_valid_.query('userid in @test_users_val').sort_values('userid')\n",
    "    holdout_valid = holdout_valid_.query('userid in @test_users_val').sort_values('userid')\n",
    "\n",
    "    test_users = np.intersect1d(testset_.userid.unique(), holdout_.userid.unique())\n",
    "    testset = testset_.query('userid in @test_users').sort_values('userid')\n",
    "    holdout = holdout_.query('userid in @test_users').sort_values('userid')\n",
    "\n",
    "\n",
    "    assert holdout_valid.set_index('userid')['timestamp'].ge(\n",
    "        testset_valid\n",
    "        .groupby('userid')\n",
    "        ['timestamp'].max()\n",
    "    ).all()\n",
    "\n",
    "    data_description = dict(\n",
    "        users = data_index['users'].name,\n",
    "        items = data_index['items'].name,\n",
    "        feedback = 'rating',\n",
    "        n_users = len(data_index['users']),\n",
    "        n_items = len(data_index['items']),\n",
    "        n_ratings = training['rating'].nunique(),\n",
    "        min_rating = training['rating'].min(),\n",
    "        test_users = holdout_valid[data_index['users'].name].drop_duplicates().values, # NEW\n",
    "        n_test_users = holdout_valid[data_index['users'].name].nunique() # NEW\n",
    "    )\n",
    "\n",
    "    return training, testset_valid, holdout_valid, testset, holdout, data_description, data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 177 invalid observations.\n"
     ]
    }
   ],
   "source": [
    "training, testset_valid, holdout_valid, testset, holdout, data_description, data_index = full_preproccessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJOfzHI5dSaz"
   },
   "source": [
    "## Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:11:30.579128Z",
     "start_time": "2022-03-24T12:11:30.564106Z"
    },
    "id": "nXnDXyWrdSbM"
   },
   "outputs": [],
   "source": [
    "def model_evaluate(recommended_items, holdout, holdout_description, alpha=3, topn=10, dcg=False):\n",
    "    itemid = holdout_description['items']\n",
    "    rateid = holdout_description['feedback']\n",
    "    n_test_users = recommended_items.shape[0]\n",
    "    holdout_items = holdout[itemid].values\n",
    "    assert recommended_items.shape[0] == len(holdout_items)\n",
    "    \n",
    "    hits_mask = recommended_items[:, :topn] == holdout_items.reshape(-1, 1)\n",
    "    pos_mask = (holdout[rateid] >= alpha).values\n",
    "    neg_mask = (holdout[rateid] < alpha).values\n",
    "    \n",
    "    # HR calculation\n",
    "    #hr = np.sum(hits_mask.any(axis=1)) / n_test_users\n",
    "    hr_pos = np.sum(hits_mask[pos_mask].any(axis=1)) / n_test_users\n",
    "    hr_neg = np.sum(hits_mask[neg_mask].any(axis=1)) / n_test_users\n",
    "    hr = hr_pos + hr_neg\n",
    "    \n",
    "    # MRR calculation\n",
    "    hit_rank = np.where(hits_mask)[1] + 1.0\n",
    "    mrr = np.sum(1 / hit_rank) / n_test_users\n",
    "    pos_hit_rank = np.where(hits_mask[pos_mask])[1] + 1.0\n",
    "    mrr_pos = np.sum(1 / pos_hit_rank) / n_test_users\n",
    "    neg_hit_rank = np.where(hits_mask[neg_mask])[1] + 1.0\n",
    "    mrr_neg = np.sum(1 / neg_hit_rank) / n_test_users\n",
    "    \n",
    "    # Matthews correlation\n",
    "    TP = np.sum(hits_mask[pos_mask]) # + \n",
    "    FP = np.sum(hits_mask[neg_mask]) # +\n",
    "    cond = (hits_mask.sum(axis = 1) == 0)\n",
    "    FN = np.sum(cond[pos_mask])\n",
    "    TN = np.sum(cond[neg_mask])\n",
    "    N = TP+FP+TN+FN\n",
    "    S = (TP+FN)/N\n",
    "    P = (TP+FP)/N\n",
    "    C = (TP/N - S*P) / np.sqrt(P*S*(1-P)*(1-S))\n",
    "    \n",
    "    # DCG calculation\n",
    "    if dcg:\n",
    "        pos_hit_rank = np.where(hits_mask[pos_mask])[1] + 1.0\n",
    "        neg_hit_rank = np.where(hits_mask[neg_mask])[1] + 1.0\n",
    "        ndcg = np.mean(1 / np.log2(pos_hit_rank+1))\n",
    "        ndcl = np.mean(1 / np.log2(neg_hit_rank+1))\n",
    "    \n",
    "    # coverage calculation\n",
    "    n_items = holdout_description['n_items']\n",
    "    cov = np.unique(recommended_items).size / n_items\n",
    "    if dcg:\n",
    "        return hr, hr_pos, hr_neg, mrr, mrr_pos, mrr_neg, cov, C, ndcg, ndcl\n",
    "    else:\n",
    "        return hr, hr_pos, hr_neg, mrr, mrr_pos, mrr_neg, cov, C\n",
    "\n",
    "def make_prediction(tf_scores, holdout, data_description, mode, context=\"\", print_mode=True):\n",
    "    if (mode and print_mode):\n",
    "        print(f\"for context {context} evaluation ({mode}): \\n\")\n",
    "    for n in [5, 10, 20]:\n",
    "        tf_recs = topn_recommendations(tf_scores, n)\n",
    "        hr, hr_pos, hr_neg, mrr, mrr_pos, mrr_neg, cov, C = model_evaluate(tf_recs, holdout, data_description, topn=n)\n",
    "        if (print_mode):\n",
    "            print(f\"HR@{n} = {hr:.4f}, MRR@{n} = {mrr:.4f}, Coverage@{n} = {cov:.4f}\")\n",
    "            print(f\"HR_pos@{n} = {hr_pos:.4f}, HR_neg@{n} = {hr_neg:.4f}\")\n",
    "            print(f\"MRR_pos@{n} = {mrr_pos:.4f}, MRR_neg@{n} = {mrr_neg:.4f}\")\n",
    "            print(f\"Matthews@{n} = {C:.4f}\")\n",
    "            print(\"-------------------------------------\")\n",
    "        if (n == 10):\n",
    "            mrr10 = mrr\n",
    "            hr10 = hr\n",
    "            c10 = C\n",
    "    return mrr10, hr10, c10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:11:31.067902Z",
     "start_time": "2022-03-24T12:11:31.055902Z"
    },
    "id": "60I1kLNp3UxG"
   },
   "outputs": [],
   "source": [
    "def valid_mlrank(mlrank):\n",
    "    '''\n",
    "    Only allow ranks that are suitable for truncated SVD computations\n",
    "    on unfolded compressed tensor (the result of ttm product in HOOI).\n",
    "    '''\n",
    "    s, r1, r3 = mlrank\n",
    "    r2 = r1\n",
    "    return r1*r2 > r3 and r1*r3 > r2 and r2*r3 > r1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoFFee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "from scipy.special import softmax\n",
    "\n",
    "def tf_model_build(config, data, data_description, testset, holdout, attention_matrix=np.array([])):\n",
    "    userid = data_description[\"users\"]\n",
    "    itemid = data_description[\"items\"]\n",
    "    feedback = data_description[\"feedback\"]\n",
    "\n",
    "    idx = data[[userid, itemid, feedback]].values\n",
    "    idx[:, -1] = idx[:, -1] - data_description['min_rating'] # works only for integer ratings!\n",
    "    val = np.ones(idx.shape[0], dtype='f8')\n",
    "    \n",
    "    n_users = data_description[\"n_users\"]\n",
    "    n_items = data_description[\"n_items\"]\n",
    "    n_ratings = data_description[\"n_ratings\"]\n",
    "    shape = (n_users, n_items, n_ratings)\n",
    "    core_shape = config['mlrank']\n",
    "    num_iters = config[\"num_iters\"]\n",
    "    \n",
    "    if (attention_matrix.shape[0] == 0):\n",
    "        attention_matrix = form_attention_matrix(\n",
    "            data_description['n_ratings'],\n",
    "            **config['params'],\n",
    "            format = 'csr'\n",
    "        )\n",
    "        \n",
    "    attention_matrix = np.array(attention_matrix)\n",
    "\n",
    "    item_popularity = (\n",
    "        data[itemid]\n",
    "        .value_counts(sort=False)\n",
    "        .reindex(range(n_items))\n",
    "        .fillna(1)\n",
    "        .values\n",
    "    )\n",
    "    scaling_weights = get_scaling_weights(item_popularity, scaling=config[\"scaling\"])\n",
    "\n",
    "    with io.capture_output() as captured:\n",
    "        u0, u1, u2 = sa_hooi(\n",
    "            idx, val, shape, config[\"mlrank\"],\n",
    "            attention_matrix = attention_matrix,\n",
    "            scaling_weights = scaling_weights,\n",
    "            testset = testset,\n",
    "            holdout = holdout,\n",
    "            data_description = data_description,\n",
    "            max_iters = config[\"num_iters\"],\n",
    "            parallel_ttm = True,\n",
    "            randomized = config[\"randomized\"],\n",
    "            growth_tol = config[\"growth_tol\"],\n",
    "            seed = config[\"seed\"],\n",
    "            iter_callback = None,\n",
    "        )\n",
    "    \n",
    "    return u0, u1, u2, attention_matrix    \n",
    "    \n",
    "config = {\n",
    "    \"scaling\": 1,\n",
    "    \"mlrank\": (30, 30, 5),\n",
    "    \"n_ratings\": data_description['n_ratings'],\n",
    "    \"num_iters\": 5,\n",
    "    \"params\": None,\n",
    "    \"randomized\": True,\n",
    "    \"growth_tol\": 1e-4,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "def tf_scoring(params, data, data_description, context=[\"3+4+5\"]):\n",
    "    user_factors, item_factors, feedback_factors, attention_matrix = params\n",
    "    userid = data_description[\"users\"]\n",
    "    itemid = data_description[\"items\"]\n",
    "    feedback = data_description[\"feedback\"]\n",
    "\n",
    "    data = data.sort_values(userid)\n",
    "    useridx = data[userid]\n",
    "    itemidx = data[itemid].values\n",
    "    ratings = data[feedback].values\n",
    "    ratings = ratings - data_description['min_rating']\n",
    "    \n",
    "    n_users = useridx.nunique()\n",
    "    n_items = data_description['n_items']\n",
    "    n_ratings = data_description['n_ratings']\n",
    "    \n",
    "    inv_attention = solve_triangular(attention_matrix, np.eye(5), lower=True)\n",
    "    \n",
    "    tensor_outer = tensor_outer_at('cpu')\n",
    "    #matrix_softmax = softmax(inv_attention.T @ feedback_factors)\n",
    "    matrix_softmax = inv_attention.T @ feedback_factors\n",
    "    #\n",
    "    if (context == \"5\"): # make softmax \n",
    "        inv_aT_feedback = matrix_softmax[-1, :]\n",
    "    elif (context == \"4+5\"):\n",
    "        inv_aT_feedback = np.sum(matrix_softmax[-2:, :], axis=0)\n",
    "    elif (context == \"3+4+5\"):\n",
    "        inv_aT_feedback = np.sum(matrix_softmax[-3:, :], axis=0)\n",
    "    elif (context == \"2+3+4+5\"):\n",
    "        inv_aT_feedback = np.sum(matrix_softmax[-4:, :], axis=0)\n",
    "    elif (context == \"3+4+5-2-1\"):\n",
    "        inv_aT_feedback = np.sum(matrix_softmax[-3:, :], axis=0) - np.sum(matrix_softmax[:2, :], axis=0)\n",
    "        \n",
    "    scores = tensor_outer(\n",
    "        1.0,\n",
    "        item_factors,\n",
    "        attention_matrix @ feedback_factors,\n",
    "        itemidx,\n",
    "        ratings\n",
    "    )\n",
    "    scores = np.add.reduceat(scores, np.r_[0, np.where(np.diff(useridx))[0]+1]) # sort by users\n",
    "    scores = np.tensordot(\n",
    "        scores,\n",
    "        inv_aT_feedback,\n",
    "        axes=(2, 0)\n",
    "    ).dot(item_factors.T)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "from polara.evaluation.pipelines import random_grid\n",
    "\n",
    "def full_pipeline(config, training, data_description, testset_valid, holdout_valid, testset, holdout, attention_matrix):\n",
    "\n",
    "    config[\"mlrank\"] = (30, 30, 5)\n",
    "    print(\"Starting pipeline...\")\n",
    "    print(\"Training with different context in progress...\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "\n",
    "    for context in [\"5\", \"4+5\", \"3+4+5\", \"2+3+4+5\", \"3+4+5-2-1\"]:\n",
    "        tf_params = tf_model_build(config, training, data_description, testset_valid, holdout_valid, attention_matrix=attention_matrix)\n",
    "        seen_data = testset_valid\n",
    "        tf_scores = tf_scoring(tf_params, seen_data, data_description, context)\n",
    "        downvote_seen_items(tf_scores, seen_data, data_description)\n",
    "        cur_mrr, cur_hr, cur_C = make_prediction(tf_scores, holdout_valid, data_description, \"Validation\", context)\n",
    "        print(\"------------------------------------------------------\")\n",
    "\n",
    "    print(f\"Tuning model for all contexts...\\n\")\n",
    "\n",
    "    #rank_grid = []\n",
    "    #for i in range(5, 10):\n",
    "    #    rank_grid.append(2 * 2 ** i)\n",
    "    #    rank_grid.append(3 * 2 ** i)\n",
    "    \n",
    "    #rank_grid = np.array(rank_grid)\n",
    "    tf_hyper = {\n",
    "    'scaling': np.linspace(0, 2, 21),\n",
    "    'r1': np.arange(100, 220, 25),\n",
    "    #'r2': np.arange(50, 801, 25),\n",
    "    'r3': range(2, 6, 1),\n",
    "    }\n",
    "\n",
    "    grid, param_names = random_grid(tf_hyper, n=0)\n",
    "    tf_grid = [tuple(mlrank) for mlrank in grid if valid_mlrank(mlrank)]\n",
    "\n",
    "    hr_tf = {}\n",
    "    hr_pos_tf = {}\n",
    "    hr_neg_tf = {}\n",
    "    mrr_tf = {}\n",
    "    mrr_pos_tf = {}\n",
    "    mrr_neg_tf = {}\n",
    "    cov_tf = {}\n",
    "    C_tf = {}\n",
    "    \n",
    "    seen_data = testset_valid\n",
    "    \n",
    "    for mlrank in tqdm(tf_grid):\n",
    "        with io.capture_output() as captured:\n",
    "            r1, r3 = mlrank[1:]\n",
    "            cur_mlrank = tuple((r1, r1, r3))\n",
    "            config['mlrank'] = cur_mlrank\n",
    "            config['scaling'] = mlrank[0]\n",
    "            tf_params = tf_model_build(config, training, data_description, testset_valid, holdout_valid, attention_matrix=attention_matrix)\n",
    "            for context in [\"5\", \"4+5\", \"3+4+5\", \"2+3+4+5\", \"3+4+5-2-1\"]:\n",
    "                tf_scores = tf_scoring(tf_params, seen_data, data_description, context)\n",
    "                downvote_seen_items(tf_scores, seen_data, data_description)\n",
    "                tf_recs = topn_recommendations(tf_scores, topn=10)\n",
    "                \n",
    "                hr, hr_pos, hr_neg, mrr, mrr_pos, mrr_neg, cov, C = model_evaluate(tf_recs, holdout_valid, data_description, topn=10)\n",
    "                hr_tf[(context, cur_mlrank, mlrank[0])] = hr\n",
    "                hr_pos_tf[(context, cur_mlrank, mlrank[0])] = hr_pos\n",
    "                hr_neg_tf[(context, cur_mlrank, mlrank[0])] = hr_neg\n",
    "                mrr_tf[(context, cur_mlrank, mlrank[0])] = mrr\n",
    "                mrr_pos_tf[(context, cur_mlrank, mlrank[0])] = mrr_pos\n",
    "                mrr_neg_tf[(context, cur_mlrank, mlrank[0])] = mrr_neg\n",
    "                cov_tf[(context, cur_mlrank, mlrank[0])] = cov\n",
    "                C_tf[(context, cur_mlrank, mlrank[0])] = C\n",
    "\n",
    "    print(f'Best HR={pd.Series(hr_tf).max():.4f} achieved with context {pd.Series(hr_tf).idxmax()[0]} and mlrank = {pd.Series(hr_tf).idxmax()[1]} and scale factor = {pd.Series(hr_tf).idxmax()[2]}')\n",
    "    print(f'Best HR_pos={pd.Series(hr_pos_tf).max():.4f} achieved with context {pd.Series(hr_pos_tf).idxmax()[0]} and mlrank = {pd.Series(hr_pos_tf).idxmax()[1]} and scale factor = {pd.Series(hr_pos_tf).idxmax()[2]}')\n",
    "    print(f'Best HR_neg={pd.Series(hr_neg_tf).min():.4f} achieved with context {pd.Series(hr_neg_tf).idxmin()[0]} and mlrank = {pd.Series(hr_neg_tf).idxmin()[1]} and scale factor = {pd.Series(hr_neg_tf).idxmin()[2]}')\n",
    "    \n",
    "    print(f'Best MRR={pd.Series(mrr_tf).max():.4f} achieved with context {pd.Series(mrr_tf).idxmax()[0]} and mlrank = {pd.Series(mrr_tf).idxmax()[1]} and scale factor = {pd.Series(mrr_tf).idxmax()[2]}')\n",
    "    print(f'Best MRR_pos={pd.Series(mrr_pos_tf).max():.4f} achieved with context {pd.Series(mrr_pos_tf).idxmax()[0]} and mlrank = {pd.Series(mrr_pos_tf).idxmax()[1]} and scale factor = {pd.Series(mrr_pos_tf).idxmax()[2]}')\n",
    "    print(f'Best MRR_neg={pd.Series(mrr_neg_tf).min():.4f} achieved with context {pd.Series(mrr_neg_tf).idxmin()[0]} and mlrank = {pd.Series(mrr_neg_tf).idxmin()[1]} and scale factor = {pd.Series(mrr_neg_tf).idxmin()[2]}')\n",
    "    \n",
    "    print(f'Best Matthews={pd.Series(C_tf).max():.4f} achieved with context {pd.Series(C_tf).idxmax()[0]} and mlrank = {pd.Series(C_tf).idxmax()[1]} and scale factor = {pd.Series(C_tf).idxmax()[2]}')\n",
    "                          \n",
    "    print(f'COV={pd.Series(cov_tf)[pd.Series(C_tf).idxmax()]:.4f} (based on best Matthews value)')\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    print(\"Evaluation of the best model on test holdout in progress...\\n\")\n",
    "    \n",
    "    print(\"Best by MRR@10:\\n\")\n",
    "    config[\"mlrank\"] = pd.Series(mrr_pos_tf).idxmax()[1]\n",
    "    tf_params = tf_model_build(config, training, data_description, testset, holdout, attention_matrix=attention_matrix)\n",
    "\n",
    "    seen_data = testset\n",
    "    tf_scores = tf_scoring(tf_params, seen_data, data_description, pd.Series(mrr_pos_tf).idxmax()[0])\n",
    "    downvote_seen_items(tf_scores, seen_data, data_description)\n",
    "    cur_mrr, cur_hr, cur_C = make_prediction(tf_scores, holdout, data_description, \"Test\", pd.Series(mrr_pos_tf).idxmax()[0])\n",
    "    \n",
    "    print(\"---------------------------------------------------------\")\n",
    "    \n",
    "    print(\"Best by HR@10:\\n\")\n",
    "    config[\"mlrank\"] = pd.Series(hr_pos_tf).idxmax()[1]\n",
    "    tf_params = tf_model_build(config, training, data_description, testset, holdout, attention_matrix=attention_matrix)\n",
    "\n",
    "    seen_data = testset\n",
    "    tf_scores = tf_scoring(tf_params, seen_data, data_description, pd.Series(hr_pos_tf).idxmax()[0])\n",
    "    downvote_seen_items(tf_scores, seen_data, data_description)\n",
    "    cur_mrr, cur_hr, cur_C = make_prediction(tf_scores, holdout, data_description, \"Test\", pd.Series(hr_pos_tf).idxmax()[0])\n",
    "    \n",
    "    print(\"---------------------------------------------------------\")\n",
    "    \n",
    "    print(\"Best by Matthews@10:\\n\")\n",
    "    config[\"mlrank\"] = pd.Series(C_tf).idxmax()[1]\n",
    "    tf_params = tf_model_build(config, training, data_description, testset, holdout, attention_matrix=attention_matrix)\n",
    "\n",
    "    seen_data = testset\n",
    "    tf_scores = tf_scoring(tf_params, seen_data, data_description, pd.Series(C_tf).idxmax()[0])\n",
    "    downvote_seen_items(tf_scores, seen_data, data_description)\n",
    "    cur_mrr, cur_hr, cur_C = make_prediction(tf_scores, holdout, data_description, \"Test\", pd.Series(C_tf).idxmax()[0])\n",
    "    print(\"Pipeline ended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_matrix = np.eye(5)\n",
    "full_pipeline(config, training, data_description, testset_valid, holdout_valid, testset, holdout, attention_matrix=attention_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EW7SFRIqqle7"
   },
   "source": [
    "# Random Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:11:44.551668Z",
     "start_time": "2022-03-24T12:11:44.536651Z"
    },
    "id": "KMEOnT4sqoTq"
   },
   "outputs": [],
   "source": [
    "def build_random_model(trainset, trainset_description):\n",
    "    itemid = trainset_description['items']\n",
    "    n_items = trainset[itemid].max() + 1\n",
    "    random_state = np.random.RandomState(42)\n",
    "    return n_items, random_state\n",
    "\n",
    "def random_model_scoring(params, testset, testset_description):\n",
    "    n_items, random_state = params\n",
    "    n_users = testset_description['n_test_users']\n",
    "    scores = random_state.rand(n_users, n_items)\n",
    "    return scores\n",
    "\n",
    "def simple_model_recom_func(scores, topn=20):\n",
    "    recommendations = np.apply_along_axis(topidx, 1, scores, topn)\n",
    "    return recommendations\n",
    "\n",
    "def topidx(a, topn):\n",
    "    parted = np.argpartition(a, -topn)[-topn:]\n",
    "    return parted[np.argsort(-a[parted])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQaaMX1erOcc"
   },
   "source": [
    "# Popularity-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:11:45.116079Z",
     "start_time": "2022-03-24T12:11:45.108082Z"
    },
    "id": "oI3UZ4KMrR4i"
   },
   "outputs": [],
   "source": [
    "def build_popularity_model(trainset, trainset_description):\n",
    "    itemid = trainset_description['items']\n",
    "    item_popularity = trainset[itemid].value_counts()\n",
    "    return item_popularity\n",
    "\n",
    "def popularity_model_scoring(params, testset, testset_description):\n",
    "    item_popularity = params\n",
    "    n_items = item_popularity.index.max() + 1\n",
    "    n_users = testset_description['n_test_users']\n",
    "    # fill in popularity scores for each item with indices from 0 to n_items-1\n",
    "    popularity_scores = np.zeros(n_items,)\n",
    "    popularity_scores[item_popularity.index] = item_popularity.values\n",
    "    # same scores for each test user\n",
    "    scores = np.tile(popularity_scores, n_users).reshape(n_users, n_items)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-c7NJyjoxbVF"
   },
   "source": [
    "# PureSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:11:46.267532Z",
     "start_time": "2022-03-24T12:11:46.254529Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHfx8WVYvJsQ",
    "outputId": "8791e807-11c9-41a1-c3a9-7b68cf85822d"
   },
   "outputs": [],
   "source": [
    "def matrix_from_observations(data, data_description):\n",
    "    useridx = data[data_description['users']]\n",
    "    itemidx = data[data_description['items']]\n",
    "    values = data[data_description['feedback']]\n",
    "    return csr_matrix((values, (useridx, itemidx)), dtype='f8')\n",
    "\n",
    "\n",
    "def build_svd_model(config, data, data_description):\n",
    "    source_matrix = matrix_from_observations(data, data_description)\n",
    "    D = norm(source_matrix, axis=0)\n",
    "    A = source_matrix.dot(diags(D**(config['f']-1)))\n",
    "    _, _, vt = svds(A, k=config['rank'], return_singular_vectors='vh')\n",
    "#     singular_values = s[::-1]\n",
    "    item_factors = np.ascontiguousarray(vt[::-1, :].T)\n",
    "    return item_factors\n",
    "\n",
    "def svd_model_scoring(params, data, data_description):\n",
    "    item_factors = params\n",
    "    test_data = data.assign(\n",
    "        userid = pd.factorize(data['userid'])[0]\n",
    "    )\n",
    "    test_matrix = matrix_from_observations(test_data, data_description)\n",
    "    scores = test_matrix.dot(item_factors) @ item_factors.T\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkKi34mwyWUo"
   },
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T10:15:25.329453Z",
     "start_time": "2022-03-24T10:15:25.314464Z"
    }
   },
   "outputs": [],
   "source": [
    "rank_grid = []\n",
    "for i in range(5, 10):\n",
    "    rank_grid.append(2 * 2 ** i)\n",
    "    rank_grid.append(3 * 2 ** i)\n",
    "    \n",
    "rank_grid = np.array(rank_grid)\n",
    "\n",
    "#rank_grid = np.arange(50, 601, 25)\n",
    "f_grid = np.linspace(0, 2, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T10:38:55.267179Z",
     "start_time": "2022-03-24T10:18:22.838941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97c45e0b8b6434ca42067f31e4de4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hr_tf = {}\n",
    "mrr_tf = {}\n",
    "C_tf = {}\n",
    "grid = list(zip(np.meshgrid(rank_grid, f_grid)[0].flatten(), np.meshgrid(rank_grid, f_grid)[1].flatten()))\n",
    "for params in tqdm(grid):\n",
    "    r, f = params\n",
    "    svd_config = {'rank': int(r), 'f': f}\n",
    "    svd_params = build_svd_model(svd_config, training, data_description)\n",
    "    svd_scores = svd_model_scoring(svd_params, testset_valid, data_description)\n",
    "    downvote_seen_items(svd_scores, testset_valid, data_description)\n",
    "    svd_recs = topn_recommendations(svd_scores, topn=10)\n",
    "    hr, hr_pos, hr_neg, mrr, mrr_pos, mrr_neg, cov, C = model_evaluate(svd_recs, holdout_valid, data_description, alpha=3, topn=10, dcg=False)\n",
    "    hr_tf[f'r={r}, f={f:.2f}'] = hr\n",
    "    mrr_tf[f'r={r}, f={f:.2f}'] = mrr\n",
    "    C_tf[f'r={r}, f={f:.2f}'] = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T10:40:21.195015Z",
     "start_time": "2022-03-24T10:40:21.183996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r=128, f=0.80 0.09736070381231672\n",
      "r=256, f=0.60 0.0967741935483871\n",
      "r=384, f=0.40 0.09442815249266862\n",
      "r=256, f=0.50 0.093841642228739\n",
      "r=192, f=0.80 0.09325513196480939\n",
      "r=384, f=0.20 0.09266862170087976\n",
      "r=384, f=0.30 0.09266862170087976\n",
      "r=192, f=0.70 0.09266862170087976\n",
      "r=192, f=0.50 0.09208211143695015\n",
      "r=256, f=0.70 0.09208211143695015\n"
     ]
    }
   ],
   "source": [
    "hr_sorted = sorted(hr_tf, key=hr_tf.get, reverse=True)\n",
    "for i in range(10):\n",
    "    print(hr_sorted[i], hr_tf[hr_sorted[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T10:43:48.278788Z",
     "start_time": "2022-03-24T10:43:48.266777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r=192, f=0.70 0.03413675929804962\n",
      "r=192, f=0.80 0.03355723129916678\n",
      "r=192, f=0.90 0.032957687473816506\n",
      "r=192, f=0.60 0.0326707163803938\n",
      "r=192, f=0.50 0.03247358376390634\n",
      "r=256, f=0.50 0.03240096820741983\n",
      "r=192, f=0.40 0.03233114555695201\n",
      "r=128, f=0.80 0.03214285714285714\n",
      "r=512, f=0.30 0.03201275427081879\n",
      "r=128, f=0.70 0.0318644975096588\n"
     ]
    }
   ],
   "source": [
    "mrr_sorted = sorted(mrr_tf, key=mrr_tf.get, reverse=True)\n",
    "for i in range(10):\n",
    "    print(mrr_sorted[i], mrr_tf[mrr_sorted[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T10:40:42.540415Z",
     "start_time": "2022-03-24T10:40:42.522418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r=64, f=1.90 0.08595038846814079\n",
      "r=64, f=2.00 0.08380801742096403\n",
      "r=64, f=1.10 0.08350407592632614\n",
      "r=64, f=1.30 0.08073169474641102\n",
      "r=128, f=1.50 0.08057079114741801\n",
      "r=64, f=0.90 0.0793253746854852\n",
      "r=64, f=1.00 0.07790504943751907\n",
      "r=64, f=1.20 0.07709808095772532\n",
      "r=128, f=1.10 0.07567433428305435\n",
      "r=128, f=1.40 0.07428985204372635\n"
     ]
    }
   ],
   "source": [
    "C_sorted = sorted(C_tf, key=C_tf.get, reverse=True)\n",
    "for i in range(10):\n",
    "    print(C_sorted[i], C_tf[C_sorted[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:28:23.961834Z",
     "start_time": "2022-03-24T12:28:23.944826Z"
    }
   },
   "outputs": [],
   "source": [
    "data_description = dict(\n",
    "    users = data_index['users'].name,\n",
    "    items = data_index['items'].name,\n",
    "    feedback = 'rating',\n",
    "    n_users = len(data_index['users']),\n",
    "    n_items = len(data_index['items']),\n",
    "    n_ratings = training['rating'].nunique(),\n",
    "    min_rating = training['rating'].min(),\n",
    "    test_users = holdout[data_index['users'].name].drop_duplicates().values,\n",
    "    n_test_users = holdout[data_index['users'].name].nunique()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:28:25.027454Z",
     "start_time": "2022-03-24T12:28:24.845212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context  evaluation (Test): \n",
      "\n",
      "HR@5 = 0.0023, MRR@5 = 0.0012, Coverage@5 = 0.9122\n",
      "HR_pos@5 = 0.0023, HR_neg@5 = 0.0000\n",
      "MRR_pos@5 = 0.0012, MRR_neg@5 = 0.0000\n",
      "Matthews@5 = 0.0221\n",
      "-------------------------------------\n",
      "HR@10 = 0.0023, MRR@10 = 0.0012, Coverage@10 = 0.9936\n",
      "HR_pos@10 = 0.0023, HR_neg@10 = 0.0000\n",
      "MRR_pos@10 = 0.0012, MRR_neg@10 = 0.0000\n",
      "Matthews@10 = 0.0221\n",
      "-------------------------------------\n",
      "HR@20 = 0.0069, MRR@20 = 0.0015, Coverage@20 = 1.0000\n",
      "HR_pos@20 = 0.0063, HR_neg@20 = 0.0006\n",
      "MRR_pos@20 = 0.0015, MRR_neg@20 = 0.0000\n",
      "Matthews@20 = 0.0200\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rnd_params = build_random_model(training, data_description)\n",
    "rnd_scores = random_model_scoring(rnd_params, None, data_description)\n",
    "downvote_seen_items(rnd_scores, testset, data_description)\n",
    "\n",
    "_ = make_prediction(rnd_scores, holdout, data_description, mode=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:28:33.944233Z",
     "start_time": "2022-03-24T12:28:33.910186Z"
    }
   },
   "outputs": [],
   "source": [
    "pop_params = build_popularity_model(training, data_description)\n",
    "pop_scores = popularity_model_scoring(pop_params, None, data_description)\n",
    "downvote_seen_items(pop_scores, testset, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:28:34.431536Z",
     "start_time": "2022-03-24T12:28:34.320513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context  evaluation (Test): \n",
      "\n",
      "HR@5 = 0.0115, MRR@5 = 0.0066, Coverage@5 = 0.0198\n",
      "HR_pos@5 = 0.0109, HR_neg@5 = 0.0006\n",
      "MRR_pos@5 = 0.0064, MRR_neg@5 = 0.0001\n",
      "Matthews@5 = 0.0353\n",
      "-------------------------------------\n",
      "HR@10 = 0.0230, MRR@10 = 0.0079, Coverage@10 = 0.0301\n",
      "HR_pos@10 = 0.0224, HR_neg@10 = 0.0006\n",
      "MRR_pos@10 = 0.0078, MRR_neg@10 = 0.0001\n",
      "Matthews@10 = 0.0604\n",
      "-------------------------------------\n",
      "HR@20 = 0.0483, MRR@20 = 0.0097, Coverage@20 = 0.0513\n",
      "HR_pos@20 = 0.0466, HR_neg@20 = 0.0017\n",
      "MRR_pos@20 = 0.0094, MRR_neg@20 = 0.0002\n",
      "Matthews@20 = 0.0823\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "_ = make_prediction(pop_scores, holdout, data_description, mode=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PureSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T12:28:39.191316Z",
     "start_time": "2022-03-24T12:28:36.704692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'rank': 128, 'f': 0.8}, 'HR')\n",
      "for context  evaluation (Test): \n",
      "\n",
      "HR@5 = 0.0460, MRR@5 = 0.0237, Coverage@5 = 0.2097\n",
      "HR_pos@5 = 0.0414, HR_neg@5 = 0.0046\n",
      "MRR_pos@5 = 0.0218, MRR_neg@5 = 0.0019\n",
      "Matthews@5 = 0.0430\n",
      "-------------------------------------\n",
      "HR@10 = 0.0788, MRR@10 = 0.0281, Coverage@10 = 0.2716\n",
      "HR_pos@10 = 0.0730, HR_neg@10 = 0.0058\n",
      "MRR_pos@10 = 0.0261, MRR_neg@10 = 0.0021\n",
      "Matthews@10 = 0.0781\n",
      "-------------------------------------\n",
      "HR@20 = 0.1271, MRR@20 = 0.0314, Coverage@20 = 0.3553\n",
      "HR_pos@20 = 0.1133, HR_neg@20 = 0.0138\n",
      "MRR_pos@20 = 0.0288, MRR_neg@20 = 0.0026\n",
      "Matthews@20 = 0.0660\n",
      "-------------------------------------\n",
      "({'rank': 192, 'f': 0.7}, 'MRR')\n",
      "for context  evaluation (Test): \n",
      "\n",
      "HR@5 = 0.0489, MRR@5 = 0.0237, Coverage@5 = 0.2390\n",
      "HR_pos@5 = 0.0431, HR_neg@5 = 0.0058\n",
      "MRR_pos@5 = 0.0214, MRR_neg@5 = 0.0023\n",
      "Matthews@5 = 0.0338\n",
      "-------------------------------------\n",
      "HR@10 = 0.0817, MRR@10 = 0.0281, Coverage@10 = 0.3067\n",
      "HR_pos@10 = 0.0730, HR_neg@10 = 0.0086\n",
      "MRR_pos@10 = 0.0254, MRR_neg@10 = 0.0027\n",
      "Matthews@10 = 0.0539\n",
      "-------------------------------------\n",
      "HR@20 = 0.1259, MRR@20 = 0.0311, Coverage@20 = 0.4027\n",
      "HR_pos@20 = 0.1121, HR_neg@20 = 0.0138\n",
      "MRR_pos@20 = 0.0280, MRR_neg@20 = 0.0031\n",
      "Matthews@20 = 0.0647\n",
      "-------------------------------------\n",
      "({'rank': 64, 'f': 1.9}, 'MC')\n",
      "for context  evaluation (Test): \n",
      "\n",
      "HR@5 = 0.0305, MRR@5 = 0.0176, Coverage@5 = 0.1043\n",
      "HR_pos@5 = 0.0299, HR_neg@5 = 0.0006\n",
      "MRR_pos@5 = 0.0173, MRR_neg@5 = 0.0003\n",
      "Matthews@5 = 0.0726\n",
      "-------------------------------------\n",
      "HR@10 = 0.0552, MRR@10 = 0.0208, Coverage@10 = 0.1302\n",
      "HR_pos@10 = 0.0495, HR_neg@10 = 0.0058\n",
      "MRR_pos@10 = 0.0198, MRR_neg@10 = 0.0010\n",
      "Matthews@10 = 0.0447\n",
      "-------------------------------------\n",
      "HR@20 = 0.0978, MRR@20 = 0.0238, Coverage@20 = 0.1715\n",
      "HR_pos@20 = 0.0880, HR_neg@20 = 0.0098\n",
      "MRR_pos@20 = 0.0225, MRR_neg@20 = 0.0013\n",
      "Matthews@20 = 0.0644\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for_hr = sorted(hr_tf, key=hr_tf.get, reverse=True)[0]\n",
    "for_mrr = sorted(mrr_tf, key=mrr_tf.get, reverse=True)[0]\n",
    "for_mc = sorted(C_tf, key=C_tf.get, reverse=True)[0]\n",
    "\n",
    "svd_config_hr = {'rank': int(for_hr.split(\",\")[0][2:]), 'f': float(for_hr.split(\",\")[1][3:])}\n",
    "svd_config_mrr = {'rank': int(for_mrr.split(\",\")[0][2:]), 'f': float(for_mrr.split(\",\")[1][3:])}\n",
    "svd_config_mc = {'rank': int(for_mc.split(\",\")[0][2:]), 'f': float(for_mc.split(\",\")[1][3:])}\n",
    "\n",
    "svd_configs = [(svd_config_hr, \"HR\"), (svd_config_mrr, \"MRR\"), (svd_config_mc, \"MC\")]\n",
    "\n",
    "for svd_config in svd_configs:\n",
    "    print(svd_config)\n",
    "    svd_params = build_svd_model(svd_config[0], training, data_description)\n",
    "    svd_scores = svd_model_scoring(svd_params, testset, data_description)\n",
    "    downvote_seen_items(svd_scores, testset, data_description)\n",
    "\n",
    "    _ = make_prediction(svd_scores, holdout, data_description, mode=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CoFFeeProject.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "009c838d92940ae6fa3c0eca0f0908a58be7fe030119f0cd30e204cb459dcff7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.306px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
